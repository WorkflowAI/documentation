# Benchmarks

Benchmarks are a great way to compare the performance, cost and latency of different versions of your agent.

[image]

In this section, we will learn:
- what metrics are compared in benchmarks
- how to leave a first performance review
- how AI performance reviews are generated
- how to run benchmarks to compare different versions of your agent
- how to adjust how AI performance reviews are generated

## Metrics

When you are building a AI agent, you will often want to compare the performance, cost and latency of different versions of your agent.

### Performance

We define **performance** as the ability of an agent to perform a task successfully. For example, if you are building a agent that write description of images, performance is the ability of the agent to write a good description of an image. Performance is *subjective*, and depends on the task you are trying to perform. 

### Cost, latency

**Cost** (how much - in $ - you pay for the agent to perform a task) and **latency** (how long - in seconds - it takes for the agent to perform a task) are objective metrics, which are automatically computed by WorkflowAI.

## How to leave a first performance review

From the playground, run your agent and look at the outputs section. Then leave a performance review by clicking on the thumbs up or thumbs down icon.

[image]


Once you leave a performance review, WorkflowAI will use AI to generate reviews for the other remaining outputs.

## How AI performance reviews are generated

AI-generated performance reviews are a feature of WorkflowAI that uses AI to review the outputs of other versions automatically. 

For example, assuming you're building a agent that write a summary of a article, once you leave a first performance review, WorkflowAI will use AI to generate reviews for the other remaining summaries.

![AI-generated reviews](</docs/images/ai-generated-reviews.png)

